{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__all__', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__path__', '__spec__', '__version__', 'conf', 'data', 'exceptions', 'featurizers', 'models', 'nn', 'schedulers', 'types', 'utils']\n",
      "2.1.1\n"
     ]
    }
   ],
   "source": [
    "import chemprop\n",
    "\n",
    "print(dir(chemprop))\n",
    "print(chemprop.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First chemprop run\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from lightning import pytorch as pl\n",
    "import chemprop\n",
    "from chemprop import data, featurizers, models, nn\n",
    "import torch\n",
    "\n",
    "# Specify input for the model\n",
    "input_path = r\"C:\\Users\\panag\\OneDrive\\Documents\\coding\\Projects\\AIbiotics\\mycobacteria_ml_project\\training_data\\descriptors\\05_descriptors.csv\"\n",
    "num_workers = 0 # number of workers for dataloader. 0 means using main process for data loading\n",
    "smiles_column = 'SMILES' # name of the column containing SMILES strings\n",
    "target_columns = ['Hit_Miss'] # classification of activity (either 0 or 1)\n",
    "\n",
    "# load the input dataframe\n",
    "df_input = pd.read_csv(input_path)\n",
    "\n",
    "# Get SMILES and targets\n",
    "smis = df_input.loc[:, smiles_column].values\n",
    "ys = df_input.loc[:, target_columns].values\n",
    "\n",
    "# Get molecular datapoints\n",
    "all_data = [data.MoleculeDatapoint.from_smi(smi, y) for smi, y in zip(smis, ys)] # I am not fully sure what this does exactly\n",
    "\n",
    "# Perform data splitting for training, validation and testing\n",
    "# scaffold balanced is chosen in an attempt to reduce the chance of overtraining\n",
    "mols = [d.mol for d in all_data]  # RDkit Mol objects are use for structure based splits\n",
    "train_indices, val_indices, test_indices = data.make_split_indices(mols, \"scaffold_balanced\", (0.8, 0.1, 0.1))\n",
    "train_data, val_data, test_data = data.split_data_by_indices(\n",
    "    all_data, train_indices, val_indices, test_indices\n",
    ")\n",
    "\n",
    "# Use the pre-computed descriptors to create a custom MoleculeDataset (no need for re-featurization)\n",
    "train_dset = data.MoleculeDataset(train_data[0])\n",
    "val_dset = data.MoleculeDataset(val_data[0])\n",
    "test_dset = data.MoleculeDataset(test_data[0])\n",
    "\n",
    "# load the necessary dataloaders\n",
    "train_loader = data.build_dataloader(train_dset, num_workers=num_workers)\n",
    "val_loader = data.build_dataloader(val_dset, num_workers=num_workers, shuffle=False)\n",
    "test_loader = data.build_dataloader(test_dset, num_workers=num_workers, shuffle=False)\n",
    "\n",
    "# Adapting message-passing neural network inputs\n",
    "mp = nn.BondMessagePassing()  # Bond-level message passing\n",
    "agg = nn.MeanAggregation()    # Aggregation by mean (typical for molecular tasks)\n",
    "ffn = nn.BinaryClassificationFFN()  # Binary classification\n",
    "batch_norm = False  # Start without batch normalization (adjust if needed)\n",
    "metric_list = [nn.metrics.BinaryF1Score(), nn.metrics.BinaryMCCMetric(), nn.metrics.BinaryAccuracy()] \n",
    "\n",
    "# construct MPNN\n",
    "mpnn = models.MPNN(mp, agg, ffn, batch_norm, metric_list)\n",
    "\n",
    "# Ensuring proper utilization of Tensor cores\n",
    "torch.set_float32_matmul_precision('high')\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    logger=False,  # Enable logging (e.g., TensorBoard, MLflow)\n",
    "    enable_checkpointing=True,  # Save model checkpoints\n",
    "    enable_progress_bar=True,  # Show progress bar during training\n",
    "    accelerator=\"gpu\", # Use GPU\n",
    "    devices=1,  # Use 1 GPU (or CPU if no GPU is available)\n",
    "    max_epochs=50,  # Increase epochs for better convergence\n",
    "    min_epochs=10,  # Ensure at least 10 epochs are completed\n",
    "    log_every_n_steps=10,  # Log metrics every 10 steps\n",
    "    precision=\"16-mixed\",  # Use mixed precision for faster training (if GPU supports it)\n",
    "    deterministic=False,  # Ensure reproducibility\n",
    ")\n",
    "\n",
    "# Start training\n",
    "trainer.fit(mpnn, train_loader, val_loader)\n",
    "\n",
    "# Check results\n",
    "results = trainer.test(mpnn, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next iteration with improved logging and more epochs\n",
    "# The model seems to select the final iteration, which is overtrained\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from lightning import pytorch as pl\n",
    "import chemprop\n",
    "from chemprop import data, featurizers, models, nn\n",
    "import torch\n",
    "import mlflow\n",
    "import mlflow.pytorch\n",
    "\n",
    "# Specify input for the model\n",
    "input_path = r\"C:\\Users\\panag\\OneDrive\\Documents\\coding\\Projects\\AIbiotics\\mycobacteria_ml_project\\training_data\\descriptors\\05_descriptors.csv\"\n",
    "num_workers = 0  # Number of workers for dataloader (0 uses the main process). I keep getting recommended to increase the number of workers, but this causes issues.\n",
    "smiles_column = 'SMILES'  # Name of the column containing SMILES strings\n",
    "target_columns = ['Hit_Miss']  # Binary classification labels (0 or 1)\n",
    "\n",
    "# Load the input dataframe\n",
    "df_input = pd.read_csv(input_path)\n",
    "\n",
    "# Extract SMILES and targets\n",
    "smis = df_input[smiles_column].values\n",
    "targets = df_input[target_columns].values\n",
    "\n",
    "# Convert to MoleculeDatapoint format\n",
    "all_data = [data.MoleculeDatapoint.from_smi(smi, y) for smi, y in zip(smis, targets)]\n",
    "\n",
    "# Scaffold-balanced train/val/test split\n",
    "mols = [d.mol for d in all_data]\n",
    "train_indices, val_indices, test_indices = data.make_split_indices(mols, \"scaffold_balanced\", (0.8, 0.1, 0.1))\n",
    "train_data, val_data, test_data = data.split_data_by_indices(all_data, train_indices, val_indices, test_indices)\n",
    "\n",
    "# Create MoleculeDataset without re-featurization\n",
    "train_dset = data.MoleculeDataset(train_data[0])\n",
    "val_dset = data.MoleculeDataset(val_data[0])\n",
    "test_dset = data.MoleculeDataset(test_data[0])\n",
    "\n",
    "# Dataloaders with batch size 64\n",
    "batch_size = 64\n",
    "train_loader = data.build_dataloader(train_dset, batch_size=batch_size, num_workers=num_workers)\n",
    "val_loader = data.build_dataloader(val_dset, batch_size=batch_size, num_workers=num_workers, shuffle=False)\n",
    "test_loader = data.build_dataloader(test_dset, batch_size=batch_size, num_workers=num_workers, shuffle=False)\n",
    "\n",
    "# Define model components\n",
    "mp = nn.AtomMessagePassing()  # Atom-level message passing\n",
    "agg = nn.MeanAggregation()  # I have to figure out why this one is ideal still\n",
    "ffn = nn.BinaryClassificationFFN(dropout=0.3)  # Dropout added for regularization\n",
    "batch_norm = True  # Enable batch normalization\n",
    "metric_list = [\n",
    "    nn.metrics.BinaryF1Score(),\n",
    "    nn.metrics.BinaryMCCMetric(),\n",
    "    nn.metrics.BinaryAccuracy()\n",
    "]\n",
    "\n",
    "# Construct the MPNN model\n",
    "mpnn = models.MPNN(mp, agg, ffn, batch_norm, metric_list)\n",
    "\n",
    "# Use mixed precision for faster training\n",
    "torch.set_float32_matmul_precision('high')\n",
    "\n",
    "# Define optimizer with weight decay\n",
    "optimizer = torch.optim.AdamW(mpnn.parameters(), lr=1e-4, weight_decay=1e-5)\n",
    "\n",
    "# Learning rate scheduler\n",
    "lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=5, factor=0.5)\n",
    "\n",
    "# Add an early stopper \n",
    "early_stopping = pl.callbacks.EarlyStopping(monitor='val_loss', patience=5, mode='min')\n",
    "\n",
    "# Initialize MLflow\n",
    "mlflow.set_experiment(\"MPNN_Training\")\n",
    "\n",
    "with mlflow.start_run():\n",
    "    mlflow.log_param(\"batch_size\", batch_size)\n",
    "    mlflow.log_param(\"learning_rate\", 1e-4)\n",
    "    mlflow.log_param(\"weight_decay\", 1e-5)\n",
    "    \n",
    "    # Define PyTorch Lightning trainer\n",
    "    trainer = pl.Trainer(\n",
    "        logger=pl.loggers.MLFlowLogger(experiment_name=\"MPNN_Training\"),\n",
    "        enable_checkpointing=True,  # Save model checkpoints\n",
    "        enable_progress_bar=True,  # Show progress bar during training\n",
    "        accelerator=\"gpu\",  # Use GPU if available\n",
    "        devices=1,  # Number of GPUs (or CPU fallback)\n",
    "        max_epochs=100,  # Increased training duration\n",
    "        min_epochs=10,  # Minimum epochs before early stopping\n",
    "        log_every_n_steps=1,  # Log metrics frequently\n",
    "        precision=\"16-mixed\",  # Mixed precision for efficiency\n",
    "        deterministic=True,  # Ensure reproducibility\n",
    "        gradient_clip_val=1.0,  # Prevent exploding gradients\n",
    "    )\n",
    "\n",
    "    # Train the model\n",
    "    trainer.fit(mpnn, train_loader, val_loader)\n",
    "\n",
    "    # Test the trained model\n",
    "    results = trainer.test(mpnn, test_loader)\n",
    "    \n",
    "    # Log results to MLflow\n",
    "    for metric, value in results[0].items():\n",
    "        mlflow.log_metric(metric, value)\n",
    "    \n",
    "    # Save the trained model to MLflow\n",
    "    mlflow.pytorch.log_model(mpnn, \"mpnn_model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run 3: model stopped early due to not enough learning occurring over the generations.abs\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from lightning import pytorch as pl\n",
    "import chemprop\n",
    "from chemprop import data, featurizers, models, nn\n",
    "import torch\n",
    "import mlflow\n",
    "import mlflow.pytorch\n",
    "\n",
    "# Specify input for the model\n",
    "input_path = r\"C:\\Users\\panag\\OneDrive\\Documents\\coding\\Projects\\AIbiotics\\mycobacteria_ml_project\\training_data\\descriptors\\05_descriptors.csv\"\n",
    "num_workers = 0  # Number of workers for dataloader (0 uses the main process). I keep getting recommended to increase the number of workers, but this causes issues.\n",
    "smiles_column = 'SMILES'  # Name of the column containing SMILES strings\n",
    "target_columns = ['Hit_Miss']  # Binary classification labels (0 or 1)\n",
    "\n",
    "# Load the input dataframe\n",
    "df_input = pd.read_csv(input_path)\n",
    "\n",
    "# Extract SMILES and targets\n",
    "smis = df_input[smiles_column].values\n",
    "targets = df_input[target_columns].values\n",
    "\n",
    "# Convert to MoleculeDatapoint format\n",
    "all_data = [data.MoleculeDatapoint.from_smi(smi, y) for smi, y in zip(smis, targets)]\n",
    "\n",
    "# Scaffold-balanced train/val/test split\n",
    "mols = [d.mol for d in all_data]\n",
    "train_indices, val_indices, test_indices = data.make_split_indices(mols, \"scaffold_balanced\", (0.8, 0.1, 0.1))\n",
    "train_data, val_data, test_data = data.split_data_by_indices(all_data, train_indices, val_indices, test_indices)\n",
    "\n",
    "# Create MoleculeDataset without re-featurization\n",
    "train_dset = data.MoleculeDataset(train_data[0])\n",
    "val_dset = data.MoleculeDataset(val_data[0])\n",
    "test_dset = data.MoleculeDataset(test_data[0])\n",
    "\n",
    "# Dataloaders with batch size 64\n",
    "batch_size = 64\n",
    "train_loader = data.build_dataloader(train_dset, batch_size=batch_size, num_workers=num_workers)\n",
    "val_loader = data.build_dataloader(val_dset, batch_size=batch_size, num_workers=num_workers, shuffle=False)\n",
    "test_loader = data.build_dataloader(test_dset, batch_size=batch_size, num_workers=num_workers, shuffle=False)\n",
    "\n",
    "# Define model components\n",
    "mp = nn.AtomMessagePassing()  # Atom-level message passing\n",
    "agg = nn.MeanAggregation()  # I have to figure out why this one is ideal still\n",
    "ffn = nn.BinaryClassificationFFN(dropout=0.3)  # Dropout added for regularization\n",
    "batch_norm = True  # Enable batch normalization\n",
    "metric_list = [\n",
    "    nn.metrics.BinaryF1Score(),\n",
    "    nn.metrics.BinaryMCCMetric(),\n",
    "    nn.metrics.BinaryAccuracy()\n",
    "]\n",
    "\n",
    "# Construct the MPNN model\n",
    "mpnn = models.MPNN(mp, agg, ffn, batch_norm, metric_list)\n",
    "\n",
    "# Use mixed precision for faster training\n",
    "torch.set_float32_matmul_precision('high')\n",
    "\n",
    "# Define optimizer with weight decay\n",
    "optimizer = torch.optim.AdamW(mpnn.parameters(), lr=1e-4, weight_decay=1e-5)\n",
    "\n",
    "# Learning rate scheduler\n",
    "lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=5, factor=0.5)\n",
    "\n",
    "# Add an early stopper \n",
    "early_stopping = pl.callbacks.EarlyStopping(monitor='val_loss', patience=5, mode='min')\n",
    "\n",
    "# Initialize MLflow\n",
    "mlflow.set_experiment(\"MPNN_Training\")\n",
    "\n",
    "with mlflow.start_run():\n",
    "    mlflow.log_param(\"batch_size\", batch_size)\n",
    "    mlflow.log_param(\"learning_rate\", 1e-4)\n",
    "    mlflow.log_param(\"weight_decay\", 1e-5)\n",
    "\n",
    "    # Add ModelCheckpoint to callbacks\n",
    "    checkpoint_cb = pl.callbacks.ModelCheckpoint(\n",
    "        monitor=\"val_loss\",\n",
    "        mode=\"min\",\n",
    "        save_top_k=1,\n",
    "        filename=\"best_model\"\n",
    "    )\n",
    "\n",
    "    # Define PyTorch Lightning trainer\n",
    "    trainer = pl.Trainer(\n",
    "        logger=pl.loggers.MLFlowLogger(experiment_name=\"MPNN_Training\"),\n",
    "        callbacks=[early_stopping, checkpoint_cb],  # Add checkpoint callback here\n",
    "        enable_checkpointing=True,  # Save model checkpoints\n",
    "        enable_progress_bar=True,  # Show progress bar during training\n",
    "        accelerator=\"gpu\",  # Use GPU if available\n",
    "        devices=1,  # Number of GPUs (or CPU fallback)\n",
    "        max_epochs=100,  # Increased training duration\n",
    "        min_epochs=10,  # Minimum epochs before early stopping\n",
    "        log_every_n_steps=1,  # Log metrics frequently\n",
    "        precision=\"16-mixed\",  # Mixed precision for efficiency\n",
    "        deterministic=True,  # Ensure reproducibility\n",
    "        gradient_clip_val=1.0,  # Prevent exploding gradients\n",
    "    )\n",
    "\n",
    "    # Train the model\n",
    "    trainer.fit(mpnn, train_loader, val_loader)\n",
    "\n",
    "    # After training, load best model\n",
    "    best_model = models.MPNN.load_from_checkpoint(checkpoint_cb.best_model_path)\n",
    "\n",
    "    # Test the best model (not the final model)\n",
    "    results = trainer.test(best_model, test_loader)\n",
    "\n",
    "    # Log results to MLflow\n",
    "    for metric, value in results[0].items():\n",
    "        mlflow.log_metric(metric, value)\n",
    "\n",
    "    # Save the best model to MLflow\n",
    "    mlflow.pytorch.log_model(best_model, \"mpnn_model\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chemprop",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
